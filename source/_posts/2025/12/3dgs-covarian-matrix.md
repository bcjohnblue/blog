---
title: 3DGS 共變異數矩陣的作用與原理
date: 2025-12-10 22:33:35
updated: 2025-12-10 22:33:35
categories: 3DGS
mathjax: true
---

在 3D Gaussian Splatting (3DGS) 中，共變異數矩陣 $\Sigma$ 扮演著至關重要的角色。它不僅決定了每個 3D 高斯的形狀和方向，更是整個渲染過程中保證數學穩定性的關鍵。本文將從變異數的基礎概念開始，深入探討共變異數矩陣的功用、特徵，並詳細解答三個核心問題。

<!-- more -->

## 變異數 (Variance)

在學習 **共變異數** 前，我們需要先了解什麼是 **變異數**，**變異數** 的公式可用來衡量資料的分散程度，其定義是每個觀測值與平均值之差的平方的平均。簡單而言，**變異數** 等於所有數據點與其平均值差距的平方後的平均值

對於一組數據 $X = \{x_1, x_2, ..., x_n\}$，其變異數定義為：
$$\text{Var}(X) = \frac{1}{n} \sum_{i=1}^{n} (x_i - \mu)^2$$

其中：

- $x_i$ 為每個數據點
- $\mu$ 是數據的平均值
- $n$ 是數據點的個數

幾何意義：

- 變異數衡量數據點相對於平均值的「分散程度」
- 較大的變異數表示數據點較為分散
- 較小的變異數表示數據點較為集中

### 範例

假設有三個人的身高分別為 160、170、180 公分

**步驟 1：計算平均值**
$$\mu = \frac{160 + 170 + 180}{3} = \frac{510}{3} = 170$$

**步驟 2：計算變異數**

{% raw %}

$$
\begin{align*}
\text{Var}(X) &= \frac{(160-170)^2 + (170-170)^2 + (180-170)^2}{3} \\
&= \frac{(-10)^2 + 0^2 + 10^2}{3} = \frac{100 + 0 + 100}{3} \\
&= \frac{200}{3} \approx 66.67
\end{align*}
$$

{% endraw %}

因此，變異數約為 66.67，代表這三個身高數據的分散程度

## 共變異數 (Covariance)

**共變異數** 用以衡量兩個變數同時變動的方向與程度。它的定義是兩變數偏離各自平均數的乘積的平均。

共變異數公式為：
$$\text{Cov}(X,Y) = \frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})$$

其中：

- $x_i, y_i$ 分別為兩變數的觀測值
- $\bar{x}, \bar{y}$ 分別是兩變數的平均值
- $n$ 是資料對數

幾何意義：

- **正值**：兩變數同向變動（正相關）
- **負值**：兩變數反向變動（負相關）
- **接近零**：兩變數無明顯線性關係

### 範例

假設有一組學生的邏輯能力分數（X）和他們的課程成績（Y）如下：

| 學生 | X (邏輯能力) | Y (課程成績) |
| ---- | ------------ | ------------ |
| 1    | 70           | 72           |
| 2    | 75           | 78           |
| 3    | 80           | 75           |
| 4    | 85           | 83           |
| 5    | 90           | 89           |

**步驟 1：計算平均值**
$$\bar{x} = \frac{70 + 75 + 80 + 85 + 90}{5} = \frac{400}{5} = 80$$
$$\bar{y} = \frac{72 + 78 + 75 + 83 + 89}{5} = \frac{397}{5} = 79.4$$

**步驟 2：計算共變異數**

{% raw %}

$$
\begin{align*}
\text{Cov}(X,Y) &= \frac{(70-80)(72-79.4) + (75-80)(78-79.4) + (80-80)(75-79.4) + (85-80)(83-79.4) + (90-80)(89-79.4)}{4} \\
&= \frac{(-10)(-7.4) + (-5)(-1.4) + 0 \times (-4.4) + 5 \times 3.6 + 10 \times 9.6}{4} \\
&= \frac{74 + 7 + 0 + 18 + 96}{4} \\
&= \frac{195}{4} = 48.75
\end{align*}
$$

{% endraw %}

結果共變異數為正（48.75），表示兩變數同向變動關係較強，即邏輯能力與成績呈正相關。

## 共變異數矩陣 (Covariance Matrix)

**共變異數** 可以描述兩個變數間的相關性，但當需要處理多維變數時，單純的共變異數已不足以描述其分布特性，我們需要一個矩陣來同時描述各維度變數之間的關係，而這就是 **共變異數矩陣 (Covariance Matrix)**

### 定義

假設有三個隨機變數：$X_1, X_2, X_3$，它們的共變異數矩陣 $\Sigma$ 是：

{% raw %}

$$
\Sigma =
\begin{pmatrix}
\text{Var}(X_1) & \text{Cov}(X_1, X_2) & \text{Cov}(X_1, X_3) \\
\text{Cov}(X_2, X_1) & \text{Var}(X_2) & \text{Cov}(X_2, X_3) \\
\text{Cov}(X_3, X_1) & \text{Cov}(X_3, X_2) & \text{Var}(X_3)
\end{pmatrix}
$$

{% endraw %}

其中：

- 對角線：各變數的變異數
- 非對角線：變數之間的共變異數

### 範例

假設我們有以下四筆樣本資料：

| 樣本 | $X_1$ | $X_2$ | $X_3$ |
| ---- | ----- | ----- | ----- |
| 1    | 2     | 3     | 1     |
| 2    | 4     | 7     | 5     |
| 3    | 6     | 5     | 2     |
| 4    | 8     | 9     | 6     |

**步驟 1：計算樣本平均**
$$\bar{X}_1 = \frac{2 + 4 + 6 + 8}{4} = 5$$
$$\bar{X}_2 = \frac{3 + 7 + 5 + 9}{4} = 6$$
$$\bar{X}_3 = \frac{1 + 5 + 2 + 6}{4} = 3.5$$

**步驟 2：計算偏差**
以樣本 1 為例：
$$(X_1 - \bar{X}_1, X_2 - \bar{X}_2, X_3 - \bar{X}_3) = (2-5, 3-6, 1-3.5) = (-3, -3, -2.5)$$

**步驟 3：計算共變異數**
使用公式：

{% raw %}

$$
\text{Cov}(X_i, X_j) = \frac{1}{n-1} \sum_{k=1}^{n} (X_{i,k} - \bar{X}_i)(X_{j,k} - \bar{X}_j)
$$

{% endraw %}

以 $\text{Var}(X_1)$ 為例：

{% raw %}

$$
\begin{align*}
\text{Var}(X_1) &= \frac{1}{3}[(2-5)^2 + (4-5)^2 + (6-5)^2 + (8-5)^2] \\
&= \frac{1}{3}[(-3)^2 + (-1)^2 + (1)^2 + (3)^2] \\
&= \frac{1}{3}[9 + 1 + 1 + 9] \\
&= \frac{1}{3} \times 20 = \frac{20}{3} \approx 6.67
\end{align*}
$$

{% endraw %}

以 $\text{Cov}(X_1, X_2)$ 為例：

{% raw %}

$$
\begin{align*}
\text{Cov}(X_1, X_2) &= \frac{1}{3}[(2-5)(3-6) + (4-5)(7-6) + (6-5)(5-6) + (8-5)(9-6)] \\
&= \frac{1}{3}[(-3)(-3) + (-1)(1) + (1)(-1) + (3)(3)] \\
&= \frac{1}{3}[9 + (-1) + (-1) + 9] \\
&= \frac{1}{3} \times 20 = \frac{20}{3} \approx 6.67
\end{align*}
$$

{% endraw %}

**步驟 4：完成矩陣計算**
經過類似計算，最終的共變異數矩陣為：

{% raw %}

$$
\Sigma \approx \begin{pmatrix}
6.67 & 6.67 & 5.83 \\
6.67 & 6.67 & 5.50 \\
5.83 & 5.50 & 6.33
\end{pmatrix}
$$

{% endraw %}

幾何意義：

- **$\text{Var}(X_i)$**：變數自身的分散程度 (對角線)
- **$\text{Cov}(X_i, X_j)$**：變數間的線性相關 (非對角線)

<!-- 在 3D 空間中，這個矩陣決定了高斯分布的「橢球」形狀：

- **對角元素**：控制橢球在各個軸向的「長度」
- **非對角元素**：控制橢球的「傾斜」程度

當非對角元素為零時，橢球的主軸與座標軸平行；當非對角元素不為零時，橢球會發生旋轉。 -->

---

## 🎯 共變異數矩陣的功用

### 1. 描述高斯的形狀

共變異數矩陣 $\Sigma$ 決定了 3D 高斯的橢球外觀，包括：

- **大小**：矩陣的特徵值決定橢球在各個主軸方向的延伸程度
- **方向**：特徵向量決定橢球的主軸方向

### 2. 控制點雲模糊範圍

在渲染過程中，共變異數矩陣影響每個高斯在空間中的「覆蓋範圍」：

- 較大的特徵值 → 在該方向有更大的模糊範圍
- 較小的特徵值 → 在該方向較為集中

### 3. 參與投影到 2D

當 3D 高斯投影到螢幕平面時，共變異數矩陣決定了在畫面上的橢圓形 footprint：
$$\Sigma_{2D} = J \Sigma_{3D} J^T$$
其中 $J$ 是雅可比矩陣，描述從 3D 到 2D 的投影變換。

### 4. 保證數學穩定性

透過適當的分解，確保矩陣始終保持合法的數學性質，避免產生不合理的高斯分布。

## 🔍 共變異數矩陣的特徵

### 對稱性

共變異數矩陣必須是對稱的：$\Sigma^T = \Sigma$

這是因為協方差的定義本身就具有對稱性：
$$\text{Cov}(X_i, X_j) = \text{Cov}(X_j, X_i)$$

### 半正定性 (SPD)

對任意向量 $v$，都有 $v^T \Sigma v \geq 0$

這保證了高斯分布在數學上是合理的，因為機率密度函數中的指數項：
$$\exp\left(-\frac{1}{2}(x-\mu)^T \Sigma^{-1} (x-\mu)\right)$$
需要 $\Sigma^{-1}$ 也是半正定的。

### 分解性

共變異數矩陣可以分解為旋轉和縮放的組合：
$$\Sigma = R \, S \, R^T$$

## 🧮 核心問題詳解

### 問題 1：為什麼分解形式是 $\Sigma = R \, S \, R^T$？

這個分解形式來自於**特徵分解（Eigendecomposition）**的幾何意義。

#### 數學推導

任何對稱矩陣都可以進行特徵分解：
$$\Sigma = Q \Lambda Q^T$$
其中：

- $Q$ 是正交矩陣，列向量為特徵向量
- $\Lambda$ 是對角矩陣，對角元素為特徵值

#### 幾何解釋

- $R$（旋轉矩陣）：將標準座標系旋轉到橢球的主軸方向
- $S$（縮放矩陣）：沿著主軸方向進行縮放
- $R^T$：將結果旋轉回原座標系

#### 視覺化理解

想像一個單位球：

1. 首先用 $R^T$ 將座標系旋轉
2. 用 $S$ 將球在各個軸向拉伸成橢球
3. 用 $R$ 將橢球旋轉到最終方向

這樣的分解確保了我們可以獨立控制：

- **形狀**：透過縮放矩陣 $S$ 的對角元素
- **方向**：透過旋轉矩陣 $R$

### 問題 2：為什麼要確保矩陣對稱且半正定？

#### 對稱性的必要性

1. **數學定義**：協方差本身就是對稱的概念
2. **計算效率**：對稱矩陣只需存儲上三角或下三角部分
3. **特徵分解**：只有對稱矩陣才能保證實數特徵值和正交特徵向量

#### 半正定性的必要性

**機率論要求**：
高斯分布的機率密度函數為：
$$p(x) = \frac{1}{(2\pi)^{3/2}|\Sigma|^{1/2}} \exp\left(-\frac{1}{2}(x-\mu)^T \Sigma^{-1} (x-\mu)\right)$$

為了讓這個函數有意義：

- $|\Sigma| > 0$（行列式為正）
- $\Sigma^{-1}$ 必須存在且半正定
- 指數項 $-\frac{1}{2}(x-\mu)^T \Sigma^{-1} (x-\mu) \leq 0$

**幾何意義**：

- 半正定保證了橢球的「內部」是定義良好的
- 如果不是半正定，會出現「鞍點」形狀，在某些方向上機率會無限增大

**數值穩定性**：

- 避免矩陣求逆時的數值問題
- 防止梯度爆炸或消失

### 問題 3：分解後如何維持半正定性？

#### 理論保證

**旋轉矩陣的性質**：

- $R$ 是正交矩陣：$R^T R = I$
- 正交變換保持向量長度：$\|Rv\| = \|v\|$

**縮放矩陣的約束**：

$$
S = \begin{pmatrix}
s_1 & 0 & 0 \\
0 & s_2 & 0 \\
0 & 0 & s_3
\end{pmatrix}
$$

只要確保 $s_1, s_2, s_3 \geq 0$，就能保證 $S$ 半正定。

#### 數學證明

對任意向量 $v$：
$$v^T \Sigma v = v^T (R S R^T) v = (R^T v)^T S (R^T v)$$

設 $u = R^T v$，則：
$$v^T \Sigma v = u^T S u = s_1 u_1^2 + s_2 u_2^2 + s_3 u_3^2$$

由於 $s_i \geq 0$ 且 $u_i^2 \geq 0$，所以 $v^T \Sigma v \geq 0$。

#### 實作上的保證

**參數化技巧**：
在 3DGS 的實作中，通常使用：

- **四元數**表示旋轉 $R$（自動保證正交性）
- **對數或平方**參數化縮放係數（確保非負）

```python
# 偽代碼示例
def build_covariance_matrix(quaternion, scale):
    # 四元數轉旋轉矩陣，自動正交
    R = quaternion_to_rotation_matrix(quaternion)

    # 使用 exp 確保縮放係數為正
    s = torch.exp(scale)  # 或使用 scale^2
    S = torch.diag(s)

    # 構建共變異數矩陣
    Sigma = R @ S @ R.T
    return Sigma
```

## 🎨 幾何直觀

### 橢球的形成過程

1. **標準球**：$\Sigma = I$（單位矩陣）
2. **縮放**：$\Sigma = S$（沿軸向拉伸）
3. **旋轉**：$\Sigma = R S R^T$（最終橢球）

### 特徵值的意義

- **大特徵值**：橢球在該方向「細長」
- **小特徵值**：橢球在該方向「扁平」
- **相等特徵值**：該方向呈現球形

## 🔧 實際應用

在 3DGS 的訓練過程中：

1. **初始化**：通常從球形高斯開始（$\Sigma = \sigma^2 I$）
2. **優化**：透過梯度下降更新四元數和縮放參數
3. **約束**：確保縮放係數始終為正
4. **投影**：將 3D 共變異數矩陣投影到 2D 螢幕空間

## 📝 總結

共變異數矩陣在 3DGS 中的重要性不僅在於其數學性質，更在於它提供了一個穩定、可控的方式來描述和操作 3D 高斯的形狀。透過 $\Sigma = R S R^T$ 的分解：

- **分離關注點**：獨立控制形狀和方向
- **保證穩定性**：維持半正定性質
- **便於優化**：提供合適的參數化方式

理解這些原理對於深入掌握 3DGS 技術至關重要，也為後續的改進和應用奠定了堅實的理論基礎。

## Reference

[機器學習 Lesson 13 — 特徵工程中的奇異值分解與共變異數的關係](https://flag-editors.medium.com/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92lesson-13-%E7%89%B9%E5%BE%B5%E5%B7%A5%E7%A8%8B%E4%B8%AD%E7%9A%84%E5%A5%87%E7%95%B0%E5%80%BC%E5%88%86%E8%A7%A3%E8%88%87%E5%85%B1%E8%AE%8A%E7%95%B0%E6%95%B8%E7%9A%84%E9%97%9C%E4%BF%82-21cf2b7a063a)
[統計課從沒搞清楚的事：算變異量為什麼要除以 n-1？什麼是「自由度」？](https://pansci.asia/archives/115065)

<style>
table {
  border-collapse: collapse;
}
th, td {
  padding: 6px 12px;
}
</style>
